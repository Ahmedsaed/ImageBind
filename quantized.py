# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s03ue8ewtMAsrII18tXrC6EOy7lN3FBD
"""
import torch
import torch.nn as nn
import torch.nn.quantizable
import torch.nn.quantized
import torch.quantization
from collections import defaultdict

# Import your ImageBind model
from imagebind.models.quantized_imagebind_model import (
    imagebind_huge,
    ModalityType,
)
from imagebind.models.transformer import (
    MultiheadAttention,
    QuantizableMultiheadAttention,
    QuantizedMultiheadAttention,
)

print("Loading ImageBind model...")
model = imagebind_huge(pretrained=True)
model.eval()


def create_dummy_data():
    """
    Creates a dummy dataset for calibration.

    Returns:
        A DataLoader with dummy data.
    """
    from torch.utils.data import Dataset, DataLoader

    class DummyDataset(Dataset):
        def __init__(self, num_samples=100):
            self.num_samples = num_samples

        def __len__(self):
            return self.num_samples

        def __getitem__(self, idx):
            # Create dummy inputs for each modality
            dummy_data = {
                ModalityType.VISION: torch.randn(3, 3, 224, 224),
                ModalityType.TEXT: torch.randint(0, 49408, (77,)),
                ModalityType.AUDIO: torch.randn(1, 128, 204),
                # ModalityType.THERMAL: torch.randn(1, 1, 2, 224, 224),
                # ModalityType.DEPTH: torch.randn(1, 1, 2, 224, 224),
                # ModalityType.IMU: torch.randn(1, 1, 2, 224, 224),
            }
            return dummy_data

    dataset = DummyDataset()
    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)
    return dataloader


print("Creating dummy data...")
data_loader = create_dummy_data()

dummy_inputs = [next(iter(data_loader)) for _ in range(10)]

print("COmputing original outputs...")
# original_outputs = []

# with torch.no_grad():
#     for sample in dummy_inputs:
#         output = model(sample)
#         original_outputs.append(output)

print("Quantizing model...")

custom_module_config = {
    "float_to_observed_custom_module_class": {
        MultiheadAttention: QuantizableMultiheadAttention
    },
    "observed_to_quantized_custom_module_class": {
        QuantizableMultiheadAttention: QuantizedMultiheadAttention
    },
}

# Prepare the model for static quantization
torch.quantization.prepare(
    model, inplace=True, prepare_custom_config_dict=custom_module_config
)

with torch.no_grad():
    for batch_idx, sample in enumerate(data_loader):
        # Forward pass for calibration
        model(sample)

        # Limit calibration to a few batches for speed
        if batch_idx >= 10:
            break

torch.quantization.convert(
    model, inplace=True, convert_custom_config_dict=custom_module_config
)

print(model)

print("Computing quantized outputs...")
quan_outputs = []

with torch.no_grad():
    for sample in dummy_inputs:
        output = model(sample)
        quan_outputs.append(output)

modality_scores = {}

for i in range(len(original_outputs)):
    original_output = original_outputs[i]
    quantized_output = quan_outputs[i]
    for modality in original_output:
        if modality in quantized_output:
            original_out = original_output[modality]
            quantized_out = quantized_output[modality]

            similarity = torch.nn.functional.cosine_similarity(
                original_out.view(1, -1), quantized_out.view(1, -1)
            ).item()

            modality_scores.setdefault(modality, []).append(similarity)

for modality, scores in modality_scores.items():
    avg_similarity = sum(scores) / len(scores)
    print(f"Average output similarity for {modality}: {avg_similarity:.6f}")

del model
