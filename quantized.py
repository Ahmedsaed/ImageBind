# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s03ue8ewtMAsrII18tXrC6EOy7lN3FBD
"""
import torch
import torch.quantization

# Import your ImageBind model
from imagebind.models.quantized_imagebind_model import (
    imagebind_huge,
    ModalityType,
)
from imagebind.models.transformer import (
    MultiheadAttention,
    QuantizableMultiheadAttention,
    QuantizedMultiheadAttention,
)
from torch.quantization.observer import (
    HistogramObserver,
    MovingAverageMinMaxObserver,
)
from torch.quantization import QConfig


print("Loading ImageBind model...")
model = imagebind_huge(pretrained=True)
model.eval()


def create_dummy_data():
    """
    Creates a dummy dataset for calibration.

    Returns:
        A DataLoader with dummy data.
    """
    from torch.utils.data import Dataset, DataLoader

    class DummyDataset(Dataset):
        def __init__(self, num_samples=100):
            self.num_samples = num_samples

        def __len__(self):
            return self.num_samples

        def __getitem__(self, idx):
            # Create dummy inputs for each modality
            dummy_data = {
                ModalityType.VISION: torch.randn(3, 224, 224),
                ModalityType.TEXT: torch.randint(0, 49408, (77,)),
                ModalityType.AUDIO: torch.randn(3, 1, 128, 204),
                ModalityType.DEPTH: torch.randn(1, 224, 224),
                ModalityType.THERMAL: torch.randn(1, 224, 224),
                ModalityType.IMU: torch.randn(6, 2000),
            }
            return dummy_data

    dataset = DummyDataset()
    dataloader = DataLoader(dataset, batch_size=10, shuffle=False)
    return dataloader


print("Creating dummy data...")
data_loader = create_dummy_data()

dummy_inputs = [next(iter(data_loader)) for _ in range(10)]

print("Computing original outputs...")
original_outputs = []

with torch.no_grad():
    for sample in dummy_inputs:
        output = model(sample)
        original_outputs.append(output)

del model

print("Quantizing model...")
q_config = QConfig(
    activation=MovingAverageMinMaxObserver.with_args(quant_max=255, quant_min=0),
    weight=torch.quantization.default_per_channel_weight_observer,
)

model = imagebind_huge(pretrained=True, q_config=q_config)

custom_module_config = {
    "float_to_observed_custom_module_class": {
        MultiheadAttention: QuantizableMultiheadAttention
    },
    "observed_to_quantized_custom_module_class": {
        QuantizableMultiheadAttention: QuantizedMultiheadAttention
    },
}

# Prepare the model for static quantization
torch.quantization.prepare(
    model, inplace=True, prepare_custom_config_dict=custom_module_config
)

with torch.no_grad():
    for sample in dummy_inputs:
        # Forward pass for calibration
        model(sample)

torch.quantization.convert(
    model, inplace=True, convert_custom_config_dict=custom_module_config
)

# print(model)

print("Computing quantized outputs...")
quan_outputs = []

with torch.no_grad():
    for sample in dummy_inputs:
        output = model(sample)
        quan_outputs.append(output)

modality_scores = {}

for i in range(len(original_outputs)):
    original_output = original_outputs[i]
    quantized_output = quan_outputs[i]
    for modality in original_output:
        if modality in quantized_output:
            original_out = original_output[modality]
            quantized_out = quantized_output[modality]

            similarity = torch.nn.functional.cosine_similarity(
                original_out.view(1, -1), quantized_out.view(1, -1)
            ).item()

            modality_scores.setdefault(modality, []).append(similarity)

for modality, scores in modality_scores.items():
    avg_similarity = sum(scores) / len(scores)
    print(f"Average output similarity for {modality}: {avg_similarity:.6f}")

# Save the quantized model
quantized_model_path = ".checkpoints/static_quantized_imagebind_model.pth"

torch.save(model.state_dict(), quantized_model_path)

print(f"Quantized model saved to {quantized_model_path}")
